{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5 Starter - Time Series Analysis \n",
    "\n",
    "### Statistical Modeling to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports & Sanity Check (Do NOT Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import statsmodels.api as sm # PACF, ACF\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Viz:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "file_path = '/kaggle/input/helper/helper.py'  # full path to the file\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"helper\", file_path)\n",
    "helper = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"helper\"] = helper\n",
    "spec.loader.exec_module(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Utilities. Read the Function names at least so that you are not re-writing code\n",
    "\n",
    "* **make_submission**: Helps you convert your predictions to competition submission ready files.\n",
    "* **rmsle**: Implementation of the metric used to evaluate your score on the leaderboard.\n",
    "* **lgbm_rmsle**: Definition that can be used to do train-val type training while printing metric scores.\n",
    "* **data import**: Imports the necessary files into the notebook\n",
    "* **preprocess_holidays**: Performs some necessary cleaning on the holiday dataset\n",
    "* **preprocess_test_train**: Performs some necessary cleaning on the test and train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data (Do NOT Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# DO NOT CHANGE\n",
    "#########################\n",
    "train, test, stores, transactions, oil, holidays = helper.data_import()\n",
    "holidays, regional, national, local, events, work_day, _, _, _ = helper.preprocess_holidays(holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: EDA & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Left join transaction to train and then print the Spearman Correlation between Total Sales and Transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - q1\n",
    "merged_df = pd.DataFrame()\n",
    "print(f\"Spearman Correlation between Total Sales and Transactions:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Plot an 'ordinary least squares' trendline between transactions and sales to verify the spearman correlation value in Q1. [0.1 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Plot these line charts in the notebook:\n",
    "\n",
    "A) Transactions vs Date (all stores color coded in the same plot) \n",
    "\n",
    "B) Average monthly transactions\n",
    "\n",
    " C) Average Transactions on the days of the wee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - q3 - Plot A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - q3 - Plot B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - q3 - Plot C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Use pandas' in-build (linear) interpolation to impute the missing oil values then overlay the imputed feature over the original.\n",
    "\n",
    "Your new feature column should be called: `dcoilwtico_interpolated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate. \n",
    "\n",
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Again, left join oil on the dataframe above and report the spearman correlation between oil and sales and oil and transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlation with sales & transactions\n",
    "\n",
    "print(\"Correlation Between Oil and Sales:\")\n",
    "print(\"Correlation Between Oil and Transactions:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Report the top-3 highest negative correlations between oil and sales of a particular product family. Now think whether oil should be discarded as a feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all correlations\n",
    "\n",
    "# Report the top 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Implement the One hot encode function \n",
    "\n",
    "You just have to finish the one-hot encoder function definition for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    # One hot encoding (pandas can do it on 1 line!) \n",
    "    \n",
    "    # Store the new columns in a list\n",
    "    \n",
    "    # Replace \" \" with \"_\" in column names.\n",
    "    \n",
    "    # Return the new dataframe and all the columns (as a list)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# DO NOT CHANGE. \n",
    "# NOTE: Run this after you have implemented the one_hot_encoder function above.\n",
    "#########################\n",
    "\n",
    "# train, test = helper.preprocess_test_train(merged_df, one_hot_encoder, stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. EMA\n",
    "\n",
    "Forecast window should be >=15 days since the test set is 15 days. **For this question use 16 as the forecast window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EMAs for each family per store (pandas has an inbuilt ema function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "\n",
    "# Use the make_submission utility function provided to save a submission CSV. \n",
    "\n",
    "# Submit to competition and note your RMSLE score somewhere for this model type.\n",
    "\n",
    "# NOTE - 1: You still need to go on the right panel and click submit \n",
    "# (make_submission will NOT submit to competition -> It just makes a submission ready file)\n",
    "# NOTE - 2: Ensure that you are not overwriting your submission.csv file in subsequent cells.\n",
    "\n",
    "# Use the make_submission utility function provided to save a submission CSV.\n",
    "\n",
    "# helper.make_submission(test_preds=[], file_name=\"EMA_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. PACF and ACF\n",
    "\n",
    "Use lib sm \n",
    "\n",
    "(statsmodel.api is already imported as sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group by date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. ADF Test -> ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing technique\n",
    "This process is meant to transform the time series data to stationary, as ARIMA model only works with stationary time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute and store the diff series\n",
    "\n",
    "# 2. Drop NA or any other erroneous values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the ACF\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "##########\n",
    "# TODO: Your plot code goes here:\n",
    "##########\n",
    "\n",
    "##########\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller (ADF) test\n",
    "\n",
    "The Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or non-stationary. Stationarity is an important assumption in many time series analysis models.\n",
    "\n",
    "The ADF test evaluates the null hypothesis that the time series has a unit root, indicating non-stationarity. The alternative hypothesis is that the time series is stationary.\n",
    "\n",
    "When performing the ADF test, we obtain the ADF statistic and the p-value. The ADF statistic is a negative number and the more negative it is, the stronger the evidence against the null hypothesis. The p-value represents the probability of observing the ADF statistic or a more extreme value if the null hypothesis were true. A low p-value (below a chosen significance level, typically 0.05) indicates strong evidence against the null hypothesis and suggests that the time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perform the ADF test\n",
    "\n",
    "# 2. Extract the test statistics and p-value\n",
    "\n",
    "# 3. Print these values\n",
    "print(\"ADF Statistic:\")\n",
    "print(\"p-value:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADF statistic is (around) -11.4. This statistic is a negative value and is more negative than the critical values at common significance levels. This suggests strong evidence against the null hypothesis of a unit root, indicating that the time series is stationary.\n",
    "\n",
    "The p-values (around)  i6.76e-2121, which is a very small value close to zero. Typically, if the p-value is below a chosen significance level (e.g., 0.05), it indicates strong evidence to reject the null hypothesis. In your case, the extremely small p-value suggests strong evidence against the presence of a unit root and supports the stationarity of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Choose the right p, q and d values for your ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with appropriate p,d,q values for ARIMA\n",
    "p_arima = None\n",
    "\n",
    "d_arima = None\n",
    "\n",
    "q_arima = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get sales series as training data (np array with appropriate dtype)\n",
    "\n",
    "# 2. Using statsmodel.tsa lib. Initialize an ARIMA model with the p,d,q params you defined. \n",
    "\n",
    "# 3. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the post model fitting summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions & submit to competition using your best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11 Define a validation set. What will be the most appropriate time period for this validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the val set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your data to the appropriate dtypes, vars, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the lgb.Dataset method to intialize your dataset iterables.\n",
    "\n",
    "# 1. Make one for the train set:\n",
    "\n",
    "# 2. Make another for the val set you defined in Q13:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the dict with appropriate params:\n",
    "lgb_params = {'num_leaves': ,\n",
    "              'learning_rate': ,\n",
    "              'feature_fraction': ,\n",
    "              'max_depth': ,\n",
    "              'verbose': 20,\n",
    "              'num_boost_round': ,\n",
    "              'early_stopping_rounds': ,\n",
    "              'nthread': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the model initialization/train params)\n",
    "model = lgb.train(lgb_params, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict the sales value on your val set using the best_iteration recorded by the LGBM\n",
    "# 2. Compute and print the RMSLE on this val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pre-process your test set to appropriate format.\n",
    "# 2. Predict -> Save using make_submission -> Submit to competition\n",
    "# 3. Note your RMSLE for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out missing params for catboost appropriately here:\n",
    "catboost_params = {\n",
    "    'iterations': ,           # Number of boosting rounds\n",
    "    'learning_rate': ,        # Learning rate for gradient boosting\n",
    "    'depth': ,                   # Depth of each tree\n",
    "    'loss_function': 'RMSLE',      # Loss function (Root Mean Squared Error for regression)\n",
    "    'eval_metric': 'RMSLE',        # Evaluation metric\n",
    "    'random_seed': 42,            # Ensures reproducibility\n",
    "    'early_stopping_rounds': ,  # Stops training if no improvement after 50 rounds\n",
    "    'verbose': 100                # Prints training progress every 100 rounds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the model\n",
    "\n",
    "# 2. Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess your test data appropriately\n",
    "\n",
    "# 4. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use make_submission -> Submit to competition\n",
    "\n",
    "# 6. Note your RMSLE for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize model with random state = 42 to be consistent with CatBoost\n",
    "\n",
    "# 2. Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Make Predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. make_submission -> Submit to competition \n",
    "\n",
    "# 5. Note your RMSLE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15. Optuna for automatic hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\n",
    "    # 1. Define the parameter search space\n",
    "    \n",
    "    # 2. Create datasets (train, val) for LightGBM\n",
    "\n",
    "    # 3. Train the model\n",
    "\n",
    "    # 4. Evaluate on the validation set\n",
    "\n",
    "    # 5. Return the metric score\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Create Optuna study to minimize the objective function\n",
    "\n",
    "start = time.time()\n",
    "# 1. Create the optuna study and specify appropriate direction\n",
    "\n",
    "# 2. Optimize (pay attention to recommended trials; 50 takes too long)\n",
    "\n",
    "# 3. Get the best parameters\n",
    "\n",
    "# 4. Print them.\n",
    "# print(\"Best parameters:\", best_params)\n",
    "\n",
    "print(\"Took:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a competition submission using these parameters\n",
    "# Note these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16. Which out of the three Catboost vs LightGBM vs XGBoost provides the best score? Why do you think this model is more suited to this dataset/problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<Your answer goes here>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extra Credit Section - Achieve the lowest score\n",
    "\n",
    "### Cross Validation Strategies & Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Try different Validation sets \n",
    "# 2. Try ensembling different methods used in this assignment together"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10087583,
     "sourceId": 88073,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
